---
title: "Model building principles"
author: "HealthyR+: Practical Logistic Regression, Session 2"
date: today
format:
  pdf: default
  html:
    code-fold: true
editor: visual
---

::: callout-important
Before starting this session, make sure you've properly restarted R following these instructions: [https://argoshare.is.ed.ac.uk/healthyr_book/restart-r-regularly.htm](https://argoshare.is.ed.ac.uk/healthyr_book/restart-r-regularly.html)
:::


```{r}
#| message: false
#| warning: false

library(tidyverse)
library(broom)
library(finalfit)
library(GGally) # ggpairs function
library(pROC)   # auc statistic
theme_set(theme_bw())

heart_data = wcgs

# derived variables from previous session
heart_data = heart_data %>%
  mutate(years = (timechd/365) %>% 
           ff_label("Time (years)"),
         # CHD status at 8 years:
         chd_8yr = case_when(years >= 8 ~ "No" ,
                              years < 8 & chd == "Yes"  ~ "Yes",
                              years < 8 & chd == "No" ~ "Censored",
                              .default = "Other"
                            ) %>% 
           na_if("Censored") %>% 
           factor() %>% 
           ff_label("8-yr CHD"))

```

# Logistic regression in R

In the previous session, we used a handy function called `finalfit` that takes the variables, puts them in a logistic regression model, and outputs the results in a neat way. This should be the very last step of your analysis, for building and evaluation the model, however, you will need to know the underlying R function.

**`glm(dependent~explanatory, data, family = "binomial")`**

`glm` stands for generalised linear models (the `family` options tells glm we want to perform a logistic regression model).

This is how a simple model for our data looks (only using one explanatory variable - treatment group).

```{r}

glm(chd_8yr ~ personality_2L, data = heart_data, family = "binomial")

```

Discuss: The coefficient for type A personality is 0.7452. What is this numbers are why is it different to what we saw in the univariable column from `finalfit()` in Session 1 (we saw an odds ratio of 2.1)?

Answer: Because these are the "raw" logits, we need to exponentiate to get odds ratios (you can even do this on the calculator on your phone!):

```{r}
exp(0.7452)
```

## Accessing the results - the classical R way

Let's save the glm() result into a variable:

```{r}
my_logreg1 = glm(chd_8yr ~ personality_2L, data = heart_data, family = "binomial")
```

Ask R to print the results using the `summary()` function (base R):

```{r}
my_logreg1 %>% summary()
```

Now try to investigate the variable `my_logreg1` in the Environment tab. The reason it looks so messy is because it's a list, rather than a data frame/tibble. In the past, we would have used our knowledge about the structure to pull out value, e.g. the exponentiation of the coefficients would look like:

```{r}
my_logreg1$coefficients %>% exp()

#or saving them into a variable:
my_coefs1 = my_logreg1$coefficients %>%
  exp() %>%
  as.data.frame()

my_coefs1

```

But this means you have to know the structure/names of the model elements, and it still converts into an uncomfortable data frame. Look how `my_coefs1` has variables saved as row names rather than a proper column.

And creating confidence intervals would have looked like this:

```{r}

my_confints = my_logreg1 %>%
  confint() %>%
  as.data.frame()

my_confints %>% 
  exp()

```

Which again creates another weird data frame with row names and % in column headers. Really we want to be using variable names that **do not** have spaces and don't start with a number or we'll need to start wrapping them in \`\` (note that quotes would not work here):

```{r}
my_confints %>% 
  mutate(conf_low = exp(`2.5 %`),
         conf_high = exp(`97.5 %`))
```

## Accessing the results in an efficient way

Instead of pulling the information out one-by-one we can use `library(broom)` to combine the results in nice data frames for us:

```{r}
library(broom)

my_logreg1 %>% 
  tidy()
```

But we have to tell it that we want the `estimate` (=coefficient) exponentiated, and that we want confidence intervals calculated as well:

```{r}
my_logreg1 %>% 
  tidy(exponentiate = TRUE, conf.int = TRUE)
```

### Exercise 1

The default confidence level of `tidy()` is set at 95%. Change it to 90%.

Hint: Usually, you would press F1 on the function you are using and look at its list of arguments to find out what the option for the the confidence level is called exactly. However, because `tidy()` works differently depending on the statistical object you input, it's main Help page is very generic. The information we are looking for comes up when you press F1 on `tidy.lm`. Or you could Google "r change tidy confidence intervals").

```{r}
# your R code here...

```

# Model Assumptions

Binary logistic regression is robust to many of the assumptions which cause problems in other statistical analyses. The main assumptions are:

1.  Binary dependent variable - this is obvious, but we need to check (alive vs death from disease only; death from other causes doesn't work);

2.  Independence of observations - the observations should not be repeated measurements or matched data;

3.  Linearity of continuous explanatory variables and the log-odds outcome - take age as an example. If the outcome, say death, gets more frequent or less frequent as age rises, the model will work well. However, say children and the elderly are at high risk of death, but those in middle years are not, then the relationship is not linear. Or more correctly, it is not monotonic, meaning that the response does not only go in one direction;

4.  No multicollinearity - explanatory variables should not be highly correlated with each other.

## Linearity of continuous variables to the response

If a variable is continuous, it will be treated as if that variable has a linear relationship with the outcome (log-odds). This can be checked using these clever plots:

```{r}
heart_data = heart_data %>%
  mutate(chd_8yr.num = as.numeric(chd_8yr)-1) 

# single variable: age ~ CHD
heart_data %>% 
  ggplot(aes(age, chd_8yr.num)) + 
  geom_point(alpha = 0.1) +
  geom_smooth(method = "loess")

# multiple variables in one go:
heart_data %>% 
  pivot_longer(c(age, height, sbp, chol)) %>% 
  ggplot(aes(value, chd_8yr.num)) +
  facet_wrap(~name, scales = "free") +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "loess")



```

## Multicollinearity (correlation of explanatory variables)

The presence of two or more highly correlated variables in a regression analysis can cause problems in the results which are generated. The resulting ORs or coefficients can become unstable, which means big shifts in their size with minimal changes to the model or the underlying data. The confidence intervals around these coefficients may also be large. Definitions of the specifics differ between sources, but there are broadly two situations:

The first is when two highly correlated variables have been included in a model, sometimes referred to simply as collinearity. This can be detected by thinking about which variables may be correlated, and then checking using plotting.

The second situation is more devious. It is where collinearity exists between three or more variables, even when no one pair of variables is particularly highly correlated. To detect this, we can use a specific metric called the *variance inflation factor*.

The `ggpairs()` function from `library(GGally)` gives you all the plots you can dream of and more, but it is a lot:

```{r}
#| fig-width: 10
#| fig-height: 10
#| message: false
#| warning: false

explanatory = c("personality_2L","age", "height", "weight",
                "sbp", "chol", "smoking")

heart_data %>% 
  ggpairs(columns = explanatory)

```

So let's investigate the variables in reasonable groups/chunks.

### Continuous to continuous

```{r}
#| fig-width: 8
#| fig-height: 8
#| message: false
#| warning: false 

select_explanatory = c("age", "weight", "sbp", "chol", "smoking")

heart_data %>% 
  ggpairs(columns = select_explanatory)

```

### Continuous to categorical

We could use ggpairs again, but if we adapt our `pivot_longer()` and `facet_wrap()` combination from Session 1 we'll get clearer plots if we also limit to a single continuous variable to check against. For example, we can add `-age` to the `pivot_longer()` call so it remains in a separate column and doesn't get lumped up with everything else:

```{r}
#| fig-width: 5
#| fig-height: 3

heart_data %>% 
  select(age, personality_2L, smoking, arcus) %>% 
  pivot_longer(cols = -age) %>% 
  ggplot(aes(value, age)) +
  geom_boxplot() +
  facet_wrap(~name, scale = "free", ncol = 2) +
  coord_flip()

```

Is there collinearity between age and our three categorical explanatory variables?

### Exercise 2

Copy the code from above and change `age` to `height` (in the select(), pivot_longer() and ggplot() calls):

```{r}
#| fig-width: 5
#| fig-height: 4

# your R code here... 


```

Is there collinearity between height and our three explanatory variables?

### Categorical to categorical

```{r}
#| fig-width: 7

heart_data %>%
  select(personality_2L, smoking, arcus) %>% 
  pivot_longer(-personality_2L) %>% 
  drop_na() %>%
  ggplot(aes(value, fill = personality_2L)) + 
  geom_bar(position = "fill") +
  facet_wrap(~name, scale = "free", ncol = 1) +
  coord_flip()
```

### Exercise 3

Compare how arcus and personality relate to smoking status.

Hint: copy the code from above, in pivot_longer() and ggplot() change the variable from personality_2L to smoking.

```{r}
#| fig-width: 7
#| echo: false


# Your R code here ... 
  
```

### Exercise 4

Remember that these plots don't remind us how many observations each group has. Plot the above on an absolute scale again (like the barplots we had in Session 1). Hint: `position = "fill"` inside `geom_bar()` standardises each bar to a constant height to give proportions, try changing "fill" to "dodge" or "stack".

```{r}
#| fig-width: 7
#| echo: false

# Your R code here ...
heart_data %>%
  select(personality_2L, smoking, arcus) %>% 
  pivot_longer(-personality_2L) %>% 
  drop_na() %>%
  ggplot(aes(value, fill = personality_2L)) + 
  geom_bar(position = "stack") +
  facet_wrap(~name, scale = "free", ncol = 1) +
  coord_flip()
```

Seeing how adherence, obstruction, perforation, and extent are related, it does not make sense include all of them in the multivariate model as the algorithm won't be able to separate the effects of each on 5-year mortality.

### **Variance inflation factor**

As a final check for the presence of higher-order correlations, the variance inflation factor can be calculated for each of the terms in a final model. This is a measure of how much the variance of a particular regression coefficient is increased due to the presence of multicollinearity in the model. *GVIF* stands for generalised variance inflation factor. A common rule of thumb is that if this is greater than 5-10 for any variable, then multicollinearity may exist. The model should be further explored and the terms removed or reduced.

```{r}
#| fig-width: 7
#| echo: false

dependent = "chd_8yr"
explanatory = c("personality_2L","age", "height", "weight",
                "sbp", "chol", "smoking")

heart_data %>% 
  glmmulti(dependent, explanatory) %>%
  car::vif()

```

# Model fitting

A statistical model is a tool to understand the world. The better your model describes your data, the more useful it will be. Fitting a successful statistical model requires decisions regarding which variable to include in the model. How do we know what to include?

Models can be constructed using the following principles:

1.  As few explanatory variables should be used as possible (parsimony);

2.  Explanatory variables associated with the outcome variable in previous studies should be accounted for;

3.  Demographic variables should be included in model exploration;

4.  Population stratification should be incorporated if available;

5.  Interactions should be checked and included if influential;

6.  Final model selection should be performed using a "criterion-based approach"

-   minimise the Akaike information criterion (AIC)

-   maximise the c-statistic (area under the receiver operator curve).

We will explore these principles as we progress.

## Criterion-based model fitting

We can access measures of model fit and alter models to improve these.

`tidy()` gives us the statistics for each variable and level, to get the overall model "goodness of fit" metrics for out model we can use `glance()`:

```{r}

my_logreg1 %>% glance()

roc(my_logreg1$y, my_logreg1$fitted) %>% auc()


```

We recommend looking at two metrics:

Akaike information criterion (AIC), which should be minimised:

```{r}

my_logreg1 %>% glance() %>%  select(AIC)

```

And the c-statistic (area under the receiver operator curve), which should be maximised:

```{r}

roc(my_logreg1$y, my_logreg1$fitted) %>% auc()

```

Or we can get finalfit to include metrics in the final table:

```{r}
heart_data %>% 
  finalfit("chd_8yr", "personality_2L", metrics = TRUE)

```

## Saving different models for comparison

It's important that you know the underlying functions of finalfit in case you want to make changes to the table or use a different profiling alrgotihm etc. In technical terms, finalfit is a "wrapper" around the relevant functions introduced above (`glm`, `summary()`, `exp()`, `roc`).

We can save the last line of code into a variable:

```{r}

fit1_chd = heart_data %>% 
  finalfit("chd_8yr", "personality_2L", metrics = TRUE)

fit1     = fit1_chd[[1]]
met_fit1 = fit1_chd[[2]]

```

# Missing data

In standard logistic regression, any variable included that has missing data will result in that patient being excluded from the analysis.

It is very important to describe and characterise missing data.

```{r}
#| warning: false

dependent   = "chd_8yr"
explanatory = c("personality_2L","age", "height", "weight",
                "sbp", "chol", "smoking")

explore_missing = heart_data %>% 
  missing_compare(dependent, explanatory)

explore_missing


```

## Answers

### 1.

```{r}
my_logreg_tidy = my_logreg1 %>% 
  tidy(exponentiate = TRUE, conf.int = TRUE,
       conf.level = 0.9)

my_logreg_tidy
```

### 2.

```{r}
#| fig-width: 5
#| fig-height: 4

heart_data %>% 
  select(height, personality_2L, smoking, arcus) %>% 
  pivot_longer(cols = -height) %>% 
  ggplot(aes(value, height)) +
  geom_boxplot() +
  facet_wrap(~name, scale = "free", ncol = 2) +
  coord_flip()
```

### 3.

```{r}
#| fig-width: 6

heart_data %>%
  select(personality_2L, smoking, arcus) %>% 
  pivot_longer(-smoking) %>% 
  drop_na() %>%
  ggplot(aes(value, fill = smoking)) + 
  geom_bar(position = "fill") +
  facet_wrap(~name, scale = "free", ncol = 1) +
  coord_flip()
```

### 4.

```{r}
#| fig-width: 7
#| echo: false

heart_data %>%
  select(personality_2L, smoking, arcus) %>% 
  pivot_longer(-personality_2L) %>% 
  drop_na() %>%
  ggplot(aes(value, fill = personality_2L)) + 
  geom_bar(position = "stack") +
  facet_wrap(~name, scale = "free", ncol = 1) +
  coord_flip()
```
