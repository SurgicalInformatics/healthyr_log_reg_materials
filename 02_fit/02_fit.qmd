---
title: "Model building principles"
author: "HealthyR+: Practical Logistic Regression, Session 2"
date: today
format:
  html:
    code-fold: true
editor: visual
---

```{r}
#| message: false
#| warning: false


library(tidyverse)
library(broom)
library(finalfit)
library(GGally) # ggpairs function
library(pROC)   # auc statistic
theme_set(theme_bw())

load(here::here("mydata_modified_use.rda"))

```

# Logistic regression in R

In the previous session, we used a handy function called `finalfit` that takes the variables, puts them in a logistic regression model, and outputs the results in a neat way. This should be the very last step of your analysis, for building and evaluation the model, however, you will need to know the underlying R function.

**`glm(dependent~explanatory, data, family = "binomial")`**

`glm` stands for generalised linear models (the `family` options tells glm we want to perform a logistic regression model).

This is how a simple model for our data looks (only using one explanatory variable - treatment group).

```{r}

glm(mort_5yr ~ rx.factor, data = colon_data, family = "binomial")

```

Discuss: The coefficients for Lev and Lev+5FU are -0.059 and -0.450, respectively. What are these numbers are why are they so different to what we saw in the univariable column in Session 1 (we saw odds ratios of 0.94 and 0.64)?

Answer: Because these are the "raw" logits, we need to exponentiate to get odds ratios (you can even do this on the calculator on your phone!):

```{r}
exp(-0.058)
exp(-0.450)
```

## Accessing the results - the classical R way

Let's save the glm() result into a variable:

```{r}

my_logreg1 = glm(mort_5yr ~ rx.factor, data = colon_data, family = "binomial")

```

Ask R to print the

```{r}

my_logreg1 %>% summary()

```

Now try to investigate the variable `my_logreg1` in the Environment tab. The reason it looks so messy is because it's a list, rather than a data frame. In the past, we would have used our knowledge about the structure to pull out value, e.g. the exponentiation of the coefficients would look like:

```{r}
my_logreg1$coefficients %>% exp()

#or saving them into a variable:
my_coefs1 = my_logreg1$coefficients %>% exp() %>% as.data.frame()

my_coefs1

```

But this means you have to know the structure/names of the model elements, and it still converts into uncomfortable data frame. Look how my_coefs1 has variables saved as row names rather than a proper column.

And creating confidence intervals would have looked like this:

```{r}

my_confints = my_logreg1 %>% confint() %>% as.data.frame()

my_confints

```

Which again creates another weird data frame with row names and % in column headers. Really we want to be using variable names that **do not** have spaces in them or we'll need to start wrapping the names in \`\`:

```{r}
my_confints$`2.5 %`
```

## Accessing the results in an efficient way

Instead of pulling the information out one-by-one we can use `library(broom)` to combine the results in nice data frames for us:

```{r}
library(broom)

my_logreg1 %>% 
  tidy()

```

But we have to tell it that we want the `estimate` (=coefficient) exponentiated, and that we want confidence intervals calculated as well:

```{r}

my_logreg_tidy = my_logreg1 %>% 
  tidy(exponentiate = TRUE, conf.int = TRUE)

my_logreg_tidy

```

### Exercise 1

The default confidence level of `tidy()` is set at 95%. Change it to 90%.

Hint: Usually, you would press F1 on the function you are using and look at its list of arguments to find out what the option for the the confidence level is called exactly. However, because `tidy()` works differently depending on the statistial object you input, it's main Help page is very generic. The information we are looking for comes up when you press F1 on `tidy.lm`. Or you could just Google "r change tidy confidence intervals").

```{r}
# your R code here...


```

# Model Assumptions

Binary logistic regression is robust to many of the assumptions which cause problems in other statistical analyses. The main assumptions are:

1.  Binary dependent variable - this is obvious, but we need to check (alive, death from disease, death from other causes doesn't work);

2.  Independence of observations - the observations should not be repeated measurements or matched data;

3.  Linearity of continuous explanatory variables and the log-odds outcome - take age as an example. If the outcome, say death, gets more frequent or less frequent as age rises, the model will work well. However, say children and the elderly are at high risk of death, but those in middle years are not, then the relationship is not linear. Or more correctly, it is not monotonic, meaning that the response does not only go in one direction;

4.  No multicollinearity - explanatory variables should not be highly correlated with each other.

## Linearity of continuous variables to the response

If a variable is continuous, it will be treated as if that variable has a linear relationship with the outcome (log-odds). Take age as an example. If the outcome, say death, gets more frequent or less frequent as age rises, the model will work well. However, say children and the elderly are at high risk of death, but those in middle years are not, then there is no linear relationship. This can be checked using these clever plots.

```{r}
#| warning: false
#| message: false

colon_data = colon_data %>% mutate(mort_5yr.num = as.numeric(mort_5yr)-1) 

colon_data %>% 
  ggplot(aes(age, mort_5yr.num)) + 
  geom_point(alpha = 0.1) +
  geom_smooth(method = "loess")


colon_data %>% 
  ggplot(aes(nodes, mort_5yr.num)) + 
  geom_point(alpha = 0.1) +
  geom_smooth(method = "loess")

```

```{r}

colon_data = colon_data %>% mutate(nodes.4groups = cut(nodes, breaks = c(0, 4, 10, 33),
                                               include.lowest = TRUE))

colon_data %>% 
  ggplot(aes(as.numeric(nodes.4groups), mort_5yr.num)) + 
  geom_point(alpha = 0.1) +
  geom_smooth(method = "loess")


```

## Multicollinearity (correlation of explanatory variables)

The presence of two or more highly correlated variables in a regression analysis can cause problems in the results which are generated. The resulting ORs or coefficients can become unstable, which means big shifts in their size with minimal changes to the model or the underlying data. The confidence intervals around these coefficients may also be large. Definitions of the specifics differ between sources, but there are broadly two situations:

The first is when two highly correlated variables have been included in a model, sometimes referred to simply as collinearity. This can be detected by thinking about which variables may be correlated, and then checking using plotting.

The second situation is more devious. It is where collinearity exists between three or more variables, even when no pair of variables is particularly highly correlated. To detect this, we can use a specific metric called the *variance inflation factor*.

The `ggpairs()` function from `library(GGally)` gives you all the plots you can dream of and more, but it is a lot:

```{r}
#| fig-width: 6
#| fig-height: 6
#| message: false
#| warning: false

explanatory = c("rx.factor", "age.factor", "sex.factor",
                # "obstruct.factor", "perfor.factor", "adhere.factor",
                "loccomp.factor", "differ.factor",
                "extent.factor", "surg.factor", "nodes")

colon_data %>% 
  ggpairs(columns = explanatory)

```

So let's investigate the variables in reasonable groups/chunks.

### Continuous to continuous

```{r}
#| fig-width: 6
#| fig-height: 6
#| message: false
#| warning: false 

select_explanatory = c("age", "nodes")

colon_data %>% 
  ggpairs(columns = select_explanatory)

```

### Continuous to categorical

Let's split that up a bit and use the `pivot_longer()` and `facet_wrap()` combination as in Session 1. This time, because we want to compare everything against, for example, age, we need to add `-age` to the `pivot_longer()` call so it doesn't get lumped up with everything else:

```{r}
#| fig-width: 5
#| fig-height: 4

colon_data %>% 
  select(age, extent.factor, sex.factor, loccomp.factor, surg.factor, differ.factor) %>% 
  pivot_longer(cols= -age) %>% 
  ggplot(aes(value, age)) +
  geom_boxplot() +
  facet_wrap(~name, scale = "free", ncol = 2) +
  coord_flip()

```

### Exercise 2

Copy the code from above and change `age` to `nodes` (in the select(), pivot_longer() and ggplot() calls):

```{r}
#| fig-width: 5
#| fig-height: 4

# your R code here... 


```

### Categorical to categorical

```{r}
#| fig-width: 7


colon_data %>%
  select(extent.factor, sex.factor, loccomp.factor, surg.factor, differ.factor) %>% 
  pivot_longer(cols = c(loccomp.factor,
                       differ.factor, extent.factor, surg.factor)) %>% 
  drop_na() %>%
  ggplot(aes(value, fill = sex.factor)) + 
  geom_bar(position = "fill") +
  facet_wrap(~name, scale = "free", ncol = 2) +
  coord_flip()
  
  
```

### Exercise 3

Copy the above code and change sex to differentiation:

```{r}
#| fig-width: 7
#| echo: false


# Your R code here ... 
  
```

Run the plot for local complications:

```{r}
#| fig-width: 7
#| echo: false


colon_data %>% 
  select(extent.factor, sex.factor, loccomp.factor, surg.factor, differ.factor) %>% 
  pivot_longer(cols = c(sex.factor, differ.factor, extent.factor, surg.factor)) %>% 
  drop_na() %>%
  ggplot(aes(value, fill = loccomp.factor)) + 
  geom_bar(position = "fill") +
  facet_wrap(~name, scale = "free", ncol = 2) +
  coord_flip()
  
```

**AHA!** One of the pairs of variables does show a dependency: `extent.factor` and `loccomp.factor`. Remember, we defined any local complications as `loccomp.factor = if_else(perfor.factor == "Yes" | obstruct.factor == "Yes" | adhere.factor == "Yes", "Yes", "No")`. Let's looks at what the variables separately:

```{r}
#| fig-width: 7
#| echo: false

colon_data %>% 
  select(perfor.factor, obstruct.factor, adhere.factor, extent.factor) %>%
  pivot_longer(cols = c(perfor.factor,
                       obstruct.factor, adhere.factor))  %>% 
    drop_na() %>%
  ggplot(aes(extent.factor, fill = value)) + 
  geom_bar(position = "fill") +
  facet_wrap(~name, scale = "free", ncol = 2) +
  coord_flip()
  
```

### Exercise 4

Remember that these plots don't remind us how many observations each group has. Plot the above on an absolute scale again (like the barplots we had in Session 1). Hint: `position = "fill"` inside `geom_bar()` standardises each bar to a constant height to give proportions.

```{r}
#| fig-width: 7
#| echo: false

# Your R code here ...

```

Seeing how adherence, obstruction, perforation, and extent are related, it does not make sense include all of them in the multivariate model as the algorithm won't be able to separate the effects of each on 5-year mortality.

### **Variance inflation factor**

As a final check for the presence of higher-order correlations, the variance inflation factor can be calculated for each of the terms in a final model. This is a measure of how much the variance of a particular regression coefficient is increased due to the presence of multicollinearity in the model. *GVIF* stands for generalised variance inflation factor. A common rule of thumb is that if this is greater than 5-10 for any variable, then multicollinearity may exist. The model should be further explored and the terms removed or reduced.

```{r}
#| fig-width: 7
#| echo: false

colon_data %>% 
  glmmulti(dependent = "mort_5yr", 
           explanatory = c("extent.factor", "adhere.factor","obstruct.factor","perfor.factor", "sex.factor", "loccomp.factor", "surg.factor", "differ.factor", "age.factor", "node4.factor")) %>%
  car::vif()

```

# Model fitting

A statistical model is a tool to understand the world. The better your model describes your data, the more useful it will be. Fitting a successful statistical model requires decisions regarding which variable to include in the model. How do we know what to include?

Models can be constructed using the following principles:

1.  As few explanatory variables should be used as possible (parsimony);

2.  Explanatory variables associated with the outcome variable in previous studies should be accounted for;

3.  Demographic variables should be included in model exploration;

4.  Population stratification should be incorporated if available;

5.  Interactions should be checked and included if influential;

6.  Final model selection should be performed using a "criterion-based approach"

-   minimise the Akaike information criterion (AIC)

-   maximise the c-statistic (area under the receiver operator curve).

We will explore these principles as we progress.

## Criterion-based model fitting

We can access measures of model fit and alter models to improve these.

`tidy()` gives us the statistics for each variable and level, to get the overall model "goodness of fit" metrics for out model we can use `glance()`:

```{r}

my_logreg1 %>% glance()

roc(my_logreg1$y, my_logreg1$fitted) %>% auc()


```

We recommend looking at two metrics:

Akaike information criterion (AIC), which should be minimised:

```{r}

my_logreg1 %>% glance() %>%  select(AIC)

```

And the c-statistic (area under the receiver operator curve), which should be maximised:

```{r}

roc(my_logreg1$y, my_logreg1$fitted) %>% auc()

```

Or we can get finalfit to include metrics in the final table:

```{r}
colon_data %>% 
finalfit("mort_5yr", "rx.factor", metrics = TRUE)

```

## Saving different models for comparison

It's important that you know the underlying functions of finalfit in case you want to make changes to the table or use a different profiling alrgotihm etc. In technical terms, finalfit is a "wrapper" around the relevant functions introduced above (`glm`, `summary()`, `exp()`, `roc`).

We can save the last line of code into a variable:

```{r}

fit1_smrz = colon_data %>% 
  finalfit("mort_5yr", "rx.factor", metrics = TRUE)

fit1 = fit1_smrz[[1]]
met_fit1 = fit1_smrz[[2]]

```

# Missing data

In standard logistic regression, any variable included that has missing data will result in that patient being excluded from the analysis.

It is very important to describe and characterise missing data.

```{r}
#| warning: false

dependent   = "mort_5yr"
explanatory = c("rx.factor", "age.factor", "sex.factor",
                # "obstruct.factor", "perfor.factor", "adhere.factor",
                "loccomp.factor", "differ.factor",
                "extent.factor", "surg.factor", "nodes")
  

explore_missing = colon_data %>% 
  missing_compare(dependent, explanatory)

explore_missing


```

# Directed acyclic graph (DAG)

It is important to think about the causal pathway you are investigating, and identify any confounders and mediators.

![](dagitty-model-overview.png)

An example DAG for our dataset could be:

![](dagitty-model.png)

## Answers

### 1.

```{r}
my_logreg_tidy = my_logreg1 %>% 
  tidy(exponentiate = TRUE, conf.int = TRUE,
       conf.level = 0.9)

my_logreg_tidy
```

### 2.

```{r}
#| fig-width: 5
#| fig-height: 4

colon_data %>% 
  select(nodes, extent.factor, sex.factor, loccomp.factor, surg.factor, differ.factor) %>%
  pivot_longer(cols= -nodes) %>% 
  ggplot(aes(value, nodes)) +
  geom_boxplot() +
  facet_wrap(~name, scale = "free", ncol = 2) +
  coord_flip()


```

### 3.

```{r}
#| fig-width: 7

colon_data %>%
  select(extent.factor, sex.factor, loccomp.factor, surg.factor, differ.factor) %>% 
  pivot_longer(cols = c(loccomp.factor,sex.factor, 
                        extent.factor, surg.factor)) %>% 
  drop_na() %>%
  ggplot(aes(value, fill = differ.factor)) + 
  geom_bar(position = "fill") +
  facet_wrap(~name, scale = "free", ncol = 2) +
  coord_flip()
```

### 4.

```{r}
#| fig-width:: 7
#| echo: false


colon_data %>% 
  select(perfor.factor, obstruct.factor, adhere.factor, extent.factor) %>%
  pivot_longer(cols = c(perfor.factor,
                       obstruct.factor, adhere.factor))  %>% 
    drop_na() %>%
  ggplot(aes(extent.factor, fill = value)) + 
  geom_bar() +
  facet_wrap(~name, scale = "free", ncol = 2) +
  coord_flip()
```
